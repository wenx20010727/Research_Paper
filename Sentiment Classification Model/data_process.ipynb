{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec71187-56cf-49bb-a740-72e353ef9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['使用移动手动电动工具,外接线绝缘皮破损,应停止使用.工业/危化品类（现场）—2016版（二）电气安全6、移动用电产品、电动工具及照明1、移动使用的用电产品和I类电动工具的绝缘线，必须采用三芯(单相)或四芯(三相)多股铜芯橡套软线。', '一般工业/危化品类（现场）—2016版（一）消防检查1、防火巡查3、消防设施、器材和消防安全标志是否在位、完整；', '消防知识要加强工业/危化品类（现场）—2016版（一）消防检查2、防火检查6、重点工种人员以及其他员工消防知识的掌握情况；', '消防通道有货物摆放 清理不及时工业/危化品类（现场）—2016版（一）消防检查1、防火巡查3、消防设施、器材和消防安全标志是否在位、完整；'], 'input_ids': tensor([[ 101,  886, 4500, 4919, 1220, 2797, 1220, 4510, 1220, 2339, 1072,  117,\n",
      "         1912, 2970, 5296, 5318, 5357, 4649, 4788, 2938,  117, 2418,  977, 3632,\n",
      "          886, 4500,  119, 2339,  689,  120, 1314, 1265, 1501, 5102, 8020, 4385,\n",
      "         1767, 8021,  100, 8112, 4276, 8020,  753, 8021, 4510, 3698, 2128, 1059,\n",
      "          127,  510, 4919, 1220, 4500, 4510,  772, 1501,  510, 4510, 1220, 2339,\n",
      "         1072, 1350, 4212, 3209,  122,  510, 4919, 1220,  886, 4500, 4638, 4500,\n",
      "         4510,  772, 1501, 1469,  151, 5102, 4510, 1220, 2339, 1072, 4638, 5318,\n",
      "         5357, 5296, 8024, 2553, 7557, 7023, 4500,  676, 5708,  113, 1296, 4685,\n",
      "          114, 2772, 1724, 5708,  113,  676, 4685,  114, 1914, 5500, 7198, 5708,\n",
      "         3583, 1947, 6763, 5296,  511,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  671, 5663, 2339,  689,  120, 1314, 1265, 1501, 5102, 8020, 4385,\n",
      "         1767, 8021,  100, 8112, 4276, 8020,  671, 8021, 3867, 7344, 3466, 3389,\n",
      "          122,  510, 7344, 4125, 2337, 3389,  124,  510, 3867, 7344, 6392, 3177,\n",
      "          510, 1690, 3332, 1469, 3867, 7344, 2128, 1059, 3403, 2562, 3221, 1415,\n",
      "         1762,  855,  510, 2130, 3146, 8039,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 3867, 7344, 4761, 6399, 6206, 1217, 2487, 2339,  689,  120, 1314,\n",
      "         1265, 1501, 5102, 8020, 4385, 1767, 8021,  100, 8112, 4276, 8020,  671,\n",
      "         8021, 3867, 7344, 3466, 3389,  123,  510, 7344, 4125, 3466, 3389,  127,\n",
      "          510, 7028, 4157, 2339, 4905,  782, 1447,  809, 1350, 1071,  800, 1447,\n",
      "         2339, 3867, 7344, 4761, 6399, 4638, 2958, 2995, 2658, 1105, 8039,  102,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 3867, 7344, 6858, 6887, 3300, 6573, 4289, 3030, 3123, 3926, 4415,\n",
      "          679, 1350, 3198, 2339,  689,  120, 1314, 1265, 1501, 5102, 8020, 4385,\n",
      "         1767, 8021,  100, 8112, 4276, 8020,  671, 8021, 3867, 7344, 3466, 3389,\n",
      "          122,  510, 7344, 4125, 2337, 3389,  124,  510, 3867, 7344, 6392, 3177,\n",
      "          510, 1690, 3332, 1469, 3867, 7344, 2128, 1059, 3403, 2562, 3221, 1415,\n",
      "         1762,  855,  510, 2130, 3146, 8039,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([0, 1, 0, 0])}\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# 1是正向情感 0是负向情感\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer,TrainingArguments, BertTokenizer, BertModel, BertPreTrainedModel,BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.setrecursionlimit(3000)\n",
    "\n",
    "\n",
    "def read_data(data_dir):\n",
    "    data = pd.read_csv(data_dir)\n",
    "    data['content'] = data['content'].fillna('')\n",
    "    data['text'] = data['content']+data['level_1']+data['level_2']+data['level_3']+data['level_4']\n",
    "    return data\n",
    "\n",
    "def fill_paddings(data, maxlen):\n",
    "    '''补全句长'''\n",
    "    if len(data) < maxlen:\n",
    "        pad_len = maxlen-len(data)\n",
    "        paddings = [0 for _ in range(pad_len)]\n",
    "        data = torch.tensor(data + paddings)\n",
    "    else:\n",
    "        data = torch.tensor(data[:maxlen])\n",
    "    return data\n",
    "\n",
    "class InputDataSet():\n",
    "\n",
    "    def __init__(self,data,tokenizer,max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):  # item是索引 用来取数据\n",
    "        text = str(self.data['text'][item])\n",
    "        labels = self.data['label'][item]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        ## 手动构建\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        tokens_ids = [101] + tokens_ids + [102]\n",
    "        input_ids = fill_paddings(tokens_ids,self.max_len)\n",
    "\n",
    "        attention_mask = [1 for _ in range(len(tokens_ids))]\n",
    "        attention_mask = fill_paddings(attention_mask,self.max_len)\n",
    "\n",
    "        token_type_ids = [0 for _ in range(len(tokens_ids))]\n",
    "        token_type_ids = fill_paddings(token_type_ids,self.max_len)\n",
    "\n",
    "        return {\n",
    "            'text':text,\n",
    "            'input_ids':input_ids,\n",
    "            'attention_mask':attention_mask,\n",
    "            'token_type_ids':token_type_ids,\n",
    "            'labels':labels\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_dir = 'data/train.csv'\n",
    "    dev_dir = 'data/dev.csv'\n",
    "    model_dir = 'bert-base-chinese'\n",
    "    train = read_data(train_dir)\n",
    "    test = read_data(dev_dir)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "    train_dataset = InputDataSet(train,tokenizer=tokenizer, max_len=128)\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=4)\n",
    "    batch = next(iter(train_dataloader))\n",
    "\n",
    "    print(batch)\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['attention_mask'].shape)\n",
    "    print(batch['token_type_ids'].shape)\n",
    "    print(batch['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977e822-bcdf-4eb2-83c7-d81913c5396f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8f13a-8e37-4178-b3e0-239d01c33fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
