{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93dc83c6-3648-4e43-ae1b-06448945a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSeq were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2024-01-09 11:12:25,783][line: 147] ==> creating ./cache/logs/2024-01-09-11-12.log\n",
      "[2024-01-09 11:12:25,783][line: 147] ==> creating ./cache/logs/2024-01-09-11-12.log\n",
      "[2024-01-09 11:12:25,783][line: 147] ==> creating ./cache/logs/2024-01-09-11-12.log\n",
      "[2024-01-09 11:12:25,787][line: 42] ==>    Train batch size = 16\n",
      "[2024-01-09 11:12:25,787][line: 42] ==>    Train batch size = 16\n",
      "[2024-01-09 11:12:25,787][line: 42] ==>    Train batch size = 16\n",
      "[2024-01-09 11:12:25,789][line: 43] ==>    Total steps = 3000\n",
      "[2024-01-09 11:12:25,789][line: 43] ==>    Total steps = 3000\n",
      "[2024-01-09 11:12:25,789][line: 43] ==>    Total steps = 3000\n",
      "[2024-01-09 11:12:25,792][line: 44] ==>    Training Start!\n",
      "[2024-01-09 11:12:25,792][line: 44] ==>    Training Start!\n",
      "[2024-01-09 11:12:25,792][line: 44] ==>    Training Start!\n",
      "[2024-01-09 11:13:30,005][line: 71] ==> ====Epoch:[1/4] avg_train_loss=0.10808====\n",
      "[2024-01-09 11:13:30,005][line: 71] ==> ====Epoch:[1/4] avg_train_loss=0.10808====\n",
      "[2024-01-09 11:13:30,005][line: 71] ==> ====Epoch:[1/4] avg_train_loss=0.10808====\n",
      "[2024-01-09 11:13:30,010][line: 72] ==> ====Training epoch took: 01:04====\n",
      "[2024-01-09 11:13:30,010][line: 72] ==> ====Training epoch took: 01:04====\n",
      "[2024-01-09 11:13:30,010][line: 72] ==> ====Training epoch took: 01:04====\n",
      "[2024-01-09 11:13:30,013][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:13:30,013][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:13:30,013][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:13:32,168][line: 78] ==> ====Epoch:[1/4] avg_val_loss=0.03159 avg_val_acc=0.99135====\n",
      "[2024-01-09 11:13:32,168][line: 78] ==> ====Epoch:[1/4] avg_val_loss=0.03159 avg_val_acc=0.99135====\n",
      "[2024-01-09 11:13:32,168][line: 78] ==> ====Epoch:[1/4] avg_val_loss=0.03159 avg_val_acc=0.99135====\n",
      "[2024-01-09 11:13:32,172][line: 79] ==> ====Validation epoch took: 01:06====\n",
      "[2024-01-09 11:13:32,172][line: 79] ==> ====Validation epoch took: 01:06====\n",
      "[2024-01-09 11:13:32,172][line: 79] ==> ====Validation epoch took: 01:06====\n",
      "[2024-01-09 11:13:32,175][line: 80] ==> \n",
      "[2024-01-09 11:13:32,175][line: 80] ==> \n",
      "[2024-01-09 11:13:32,175][line: 80] ==> \n",
      "[2024-01-09 11:14:36,867][line: 71] ==> ====Epoch:[2/4] avg_train_loss=0.05045====\n",
      "[2024-01-09 11:14:36,867][line: 71] ==> ====Epoch:[2/4] avg_train_loss=0.05045====\n",
      "[2024-01-09 11:14:36,867][line: 71] ==> ====Epoch:[2/4] avg_train_loss=0.05045====\n",
      "[2024-01-09 11:14:36,873][line: 72] ==> ====Training epoch took: 01:04====\n",
      "[2024-01-09 11:14:36,873][line: 72] ==> ====Training epoch took: 01:04====\n",
      "[2024-01-09 11:14:36,873][line: 72] ==> ====Training epoch took: 01:04====\n",
      "[2024-01-09 11:14:36,876][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:14:36,876][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:14:36,876][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:14:38,898][line: 78] ==> ====Epoch:[2/4] avg_val_loss=0.01695 avg_val_acc=0.99615====\n",
      "[2024-01-09 11:14:38,898][line: 78] ==> ====Epoch:[2/4] avg_val_loss=0.01695 avg_val_acc=0.99615====\n",
      "[2024-01-09 11:14:38,898][line: 78] ==> ====Epoch:[2/4] avg_val_loss=0.01695 avg_val_acc=0.99615====\n",
      "[2024-01-09 11:14:38,903][line: 79] ==> ====Validation epoch took: 01:06====\n",
      "[2024-01-09 11:14:38,903][line: 79] ==> ====Validation epoch took: 01:06====\n",
      "[2024-01-09 11:14:38,903][line: 79] ==> ====Validation epoch took: 01:06====\n",
      "[2024-01-09 11:14:38,906][line: 80] ==> \n",
      "[2024-01-09 11:14:38,906][line: 80] ==> \n",
      "[2024-01-09 11:14:38,906][line: 80] ==> \n",
      "[2024-01-09 11:15:43,982][line: 71] ==> ====Epoch:[3/4] avg_train_loss=0.02623====\n",
      "[2024-01-09 11:15:43,982][line: 71] ==> ====Epoch:[3/4] avg_train_loss=0.02623====\n",
      "[2024-01-09 11:15:43,982][line: 71] ==> ====Epoch:[3/4] avg_train_loss=0.02623====\n",
      "[2024-01-09 11:15:43,986][line: 72] ==> ====Training epoch took: 01:05====\n",
      "[2024-01-09 11:15:43,986][line: 72] ==> ====Training epoch took: 01:05====\n",
      "[2024-01-09 11:15:43,986][line: 72] ==> ====Training epoch took: 01:05====\n",
      "[2024-01-09 11:15:43,988][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:15:43,988][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:15:43,988][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:15:46,155][line: 78] ==> ====Epoch:[3/4] avg_val_loss=0.00520 avg_val_acc=0.99808====\n",
      "[2024-01-09 11:15:46,155][line: 78] ==> ====Epoch:[3/4] avg_val_loss=0.00520 avg_val_acc=0.99808====\n",
      "[2024-01-09 11:15:46,155][line: 78] ==> ====Epoch:[3/4] avg_val_loss=0.00520 avg_val_acc=0.99808====\n",
      "[2024-01-09 11:15:46,158][line: 79] ==> ====Validation epoch took: 01:07====\n",
      "[2024-01-09 11:15:46,158][line: 79] ==> ====Validation epoch took: 01:07====\n",
      "[2024-01-09 11:15:46,158][line: 79] ==> ====Validation epoch took: 01:07====\n",
      "[2024-01-09 11:15:46,161][line: 80] ==> \n",
      "[2024-01-09 11:15:46,161][line: 80] ==> \n",
      "[2024-01-09 11:15:46,161][line: 80] ==> \n",
      "[2024-01-09 11:16:51,486][line: 71] ==> ====Epoch:[4/4] avg_train_loss=0.01425====\n",
      "[2024-01-09 11:16:51,486][line: 71] ==> ====Epoch:[4/4] avg_train_loss=0.01425====\n",
      "[2024-01-09 11:16:51,486][line: 71] ==> ====Epoch:[4/4] avg_train_loss=0.01425====\n",
      "[2024-01-09 11:16:51,490][line: 72] ==> ====Training epoch took: 01:05====\n",
      "[2024-01-09 11:16:51,490][line: 72] ==> ====Training epoch took: 01:05====\n",
      "[2024-01-09 11:16:51,490][line: 72] ==> ====Training epoch took: 01:05====\n",
      "[2024-01-09 11:16:51,492][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:16:51,492][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:16:51,492][line: 73] ==> Running Validation...\n",
      "[2024-01-09 11:16:53,682][line: 78] ==> ====Epoch:[4/4] avg_val_loss=0.00469 avg_val_acc=0.99808====\n",
      "[2024-01-09 11:16:53,682][line: 78] ==> ====Epoch:[4/4] avg_val_loss=0.00469 avg_val_acc=0.99808====\n",
      "[2024-01-09 11:16:53,682][line: 78] ==> ====Epoch:[4/4] avg_val_loss=0.00469 avg_val_acc=0.99808====\n",
      "[2024-01-09 11:16:53,686][line: 79] ==> ====Validation epoch took: 01:07====\n",
      "[2024-01-09 11:16:53,686][line: 79] ==> ====Validation epoch took: 01:07====\n",
      "[2024-01-09 11:16:53,686][line: 79] ==> ====Validation epoch took: 01:07====\n",
      "[2024-01-09 11:16:53,690][line: 80] ==> \n",
      "[2024-01-09 11:16:53,690][line: 80] ==> \n",
      "[2024-01-09 11:16:53,690][line: 80] ==> \n",
      "[2024-01-09 11:16:54,363][line: 85] ==> \n",
      "[2024-01-09 11:16:54,363][line: 85] ==> \n",
      "[2024-01-09 11:16:54,363][line: 85] ==> \n",
      "[2024-01-09 11:16:54,367][line: 86] ==>    Training Completed!\n",
      "[2024-01-09 11:16:54,367][line: 86] ==>    Training Completed!\n",
      "[2024-01-09 11:16:54,367][line: 86] ==>    Training Completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n",
      "Total training took04:28 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.optim import AdamW\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, \\\n",
    "    get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers.utils.notebook import format_time\n",
    "from modeling import BertForSeq\n",
    "from data_process import InputDataSet,read_data,fill_paddings\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(batch_size,EPOCHS):\n",
    "\n",
    "    model = BertForSeq.from_pretrained('bert-base-chinese')\n",
    "\n",
    "    train = read_data('data/train.csv')\n",
    "    val = read_data('data/dev.csv')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "    train_dataset = InputDataSet(train, tokenizer, 128)\n",
    "    val_dataset = InputDataSet(val, tokenizer, 128)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size)\n",
    "    val_dataloader = DataLoader(val_dataset,batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)  #AdamW主用\n",
    "    total_steps = len(train_dataloader) * EPOCHS  # len(dataset)*epochs / batchsize\n",
    "    #在BERT微调中，常常使用Warmup策略来在训练初期逐渐增加学习率，以更好地适应新的任务。例如，可以先使用较小的学习率进行预热（warmup），然后再进行学习率衰减。\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=total_steps)\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    log = log_creater(output_dir='./cache/logs/')\n",
    "\n",
    "    log.info(\"   Train batch size = {}\".format(batch_size))\n",
    "    log.info(\"   Total steps = {}\".format(total_steps))\n",
    "    log.info(\"   Training Start!\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_train_loss = 0\n",
    "        t0 = time.time()\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            model.zero_grad() #for循环中梯度清0\n",
    "\n",
    "            outputs = model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids,labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)  #进行梯度剪裁,防止梯度爆炸.\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_time = format_time(time.time() - t0)\n",
    "\n",
    "        log.info('====Epoch:[{}/{}] avg_train_loss={:.5f}===='.format(epoch+1,EPOCHS,avg_train_loss))\n",
    "        log.info('====Training epoch took: {:}===='.format(train_time))\n",
    "        log.info('Running Validation...')\n",
    "\n",
    "        model.eval()\n",
    "        avg_val_loss, avg_val_acc = evaluate(model, val_dataloader)\n",
    "        val_time = format_time(time.time() - t0)\n",
    "        log.info('====Epoch:[{}/{}] avg_val_loss={:.5f} avg_val_acc={:.5f}===='.format(epoch+1,EPOCHS,avg_val_loss,avg_val_acc))\n",
    "        log.info('====Validation epoch took: {:}===='.format(val_time))\n",
    "        log.info('')\n",
    "\n",
    "        if epoch == EPOCHS-1:\n",
    "            torch.save(model,'model_stu.bin')\n",
    "            print('Model Saved!')\n",
    "    log.info('')\n",
    "    log.info('   Training Completed!')\n",
    "    print('Total training took{:} (h:mm:ss)'.format(format_time(time.time() - total_t0)))\n",
    "\n",
    "def evaluate(model,val_dataloader):\n",
    "    total_val_loss = 0\n",
    "    corrects = []\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids,labels=labels)\n",
    "\n",
    "        logits = torch.argmax(outputs.logits,dim=1)\n",
    "        ## 把每个batch预测的准确率加入到一个list中\n",
    "        ## 在加入之前，preds和labels变成cpu的格式\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        labels_ids = labels.to('cpu').numpy()\n",
    "        corrects.append((preds == labels_ids).mean())  ## [0.8,0.7,0.9]\n",
    "        ## 返回loss\n",
    "        loss = outputs.loss\n",
    "        ## 把每个batch的loss加入 total_val_loss\n",
    "        ## 总共有len(val_dataloader)个batch\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    avg_val_acc = np.mean(corrects)\n",
    "\n",
    "    return avg_val_loss, avg_val_acc\n",
    "\n",
    "def log_creater(output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    log_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\n",
    "    final_log_file = os.path.join(output_dir, log_name)\n",
    "    # creat a log\n",
    "    log = logging.getLogger('train_log')\n",
    "    log.setLevel(logging.DEBUG)\n",
    "\n",
    "    # FileHandler\n",
    "    file = logging.FileHandler(final_log_file)\n",
    "    file.setLevel(logging.DEBUG)\n",
    "\n",
    "    # StreamHandler\n",
    "    stream = logging.StreamHandler()\n",
    "    stream.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '[%(asctime)s][line: %(lineno)d] ==> %(message)s')\n",
    "\n",
    "    # setFormatter\n",
    "    file.setFormatter(formatter)\n",
    "    stream.setFormatter(formatter)\n",
    "\n",
    "    # addHandler\n",
    "    log.addHandler(file)\n",
    "    log.addHandler(stream)\n",
    "\n",
    "    log.info('creating {}'.format(final_log_file))\n",
    "    return log\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(batch_size=16,EPOCHS=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba56ada-79a4-4e4b-aaf8-b4d041cfad6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
