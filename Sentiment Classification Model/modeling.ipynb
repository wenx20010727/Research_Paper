{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aedccb2-3083-47b6-b87b-03890d21d707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSeq were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5933, -0.0844],\n",
      "        [ 0.3596, -0.1484],\n",
      "        [ 0.1573, -0.2062],\n",
      "        [ 0.0780, -0.1798]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "0.4955233633518219\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from data_process import read_data,InputDataSet\n",
    "from transformers import Trainer,TrainingArguments, BertTokenizer, BertModel, BertPreTrainedModel,BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch\n",
    "\n",
    "## 做句子的分类 BertForSequence\n",
    "class BertForSeq(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self,config):  ##  config.json\n",
    "        super(BertForSeq,self).__init__(config)\n",
    "        self.config = BertConfig(config)\n",
    "        self.num_labels = 2 # 类别数目\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids,\n",
    "            attention_mask = None,\n",
    "            token_type_ids = None,\n",
    "            labels = None,\n",
    "            return_dict = None\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        ## loss损失 预测值preds\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=return_dict\n",
    "        )  ## 预测值\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        ## logits -—— softmax层的输入（0.4， 0.6）--- 1\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))  # 二分类任务 这里的参数要做view\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,  ##损失\n",
    "            logits=logits,  ##softmax层的输入，可以理解为是个概率\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ## 加载编码器和模型\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    model = BertForSeq.from_pretrained('bert-base-chinese')\n",
    "    ## 准备数据\n",
    "    dev = read_data('data/dev.csv')\n",
    "    dev_dataset = InputDataSet(dev,tokenizer=tokenizer,max_len=128)\n",
    "    dev_dataloader = DataLoader(dev_dataset,batch_size=4,shuffle=False)\n",
    "    ## 把数据做成batch\n",
    "    batch = next(iter(dev_dataloader))\n",
    "    ## 设置device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    ## 输入embedding\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    token_type_ids = batch['token_type_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    ## 预测\n",
    "    model.eval()\n",
    "    ## 得到输出\n",
    "    outputs = model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids,labels=labels)\n",
    "    ## 取输出里面的loss和logits\n",
    "    logits = outputs.logits\n",
    "    loss = outputs.loss\n",
    "\n",
    "    print(logits)\n",
    "    print(loss.item())\n",
    "\n",
    "    preds = torch.argmax(logits,dim=1)\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef885f-74b8-4346-9559-79e1ece4b59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
